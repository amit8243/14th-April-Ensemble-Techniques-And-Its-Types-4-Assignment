{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ef6f2-133b-435e-870e-265690640a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble Techniques  And Its Types-4 Assignment\n",
    "\"\"\"Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\n",
    "numerical features if necessary.\"\"\"\n",
    "Ans: To preprocess the dataset, you need to handle missing values, encode categorical variables, and scale\n",
    "numerical features if necessary. Here is a general outline of the steps you can follow:\n",
    "\n",
    "#Load the dataset into a pandas DataFrame.\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"dataset.csv\")  # Replace \"dataset.csv\" with the actual filename and path\n",
    "data.head() #top 5 rows showing\n",
    "\n",
    "#Handle missing values:\n",
    "#Identify missing values in the dataset:\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "Decide how to handle missing values based on the nature of the data:\n",
    "If there are only a few missing values in certain columns, you can consider dropping those rows or\n",
    "imputing the missing values with mean, median, or mode.\n",
    "If there are many missing values in a column, you might choose to drop that column altogether.\n",
    "Perform the chosen method to handle missing values:\n",
    "\n",
    "# Example: Impute missing values with mean\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "\n",
    "#Encode categorical variables:\n",
    "#Identify categorical variables in the dataset:\n",
    "categorical_vars = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "\n",
    "Choose an appropriate encoding method:\n",
    "One-Hot Encoding: If the categorical variable has no inherent order or hierarchy, you can use one-hot \n",
    "encoding to create binary columns for each category.\n",
    "Label Encoding: If the categorical variable has an inherent order or hierarchy, you can use label\n",
    "encoding to convert categories into numerical values.\n",
    "Apply the chosen encoding method to categorical variables:\n",
    "    \n",
    "# Example: One-Hot Encoding\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_vars)\n",
    "\n",
    "#Scale numerical features if necessary:\n",
    "#Identify numerical features in the dataset:\n",
    "    \n",
    "numerical_vars = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "#Choose an appropriate scaling method:\n",
    "Standardization: Scales the features to have zero mean and unit variance.\n",
    "Min-Max Scaling: Scales the features to a specific range, typically between 0 and 1.\n",
    "Apply the chosen scaling method to numerical features:\n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_encoded[numerical_vars] = scaler.fit_transform(data_encoded[numerical_vars])\n",
    "\n",
    "\n",
    "The resulting preprocessed dataset, data_encoded, can be used for further analysis, such as model training \n",
    "and evaluation.\n",
    "\n",
    "\"\"\"Q2. Split the dataset into a training set (70%) and a test set (30%).\"\"\"\n",
    "Ans: To split the dataset into a training set and a test set, you can use the train_test_split function\n",
    "from the scikit-learn library. Here's how you can do it:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have a preprocessed dataset named \"data_encoded\"\n",
    "\n",
    "# Splitting the features and the target variable\n",
    "# Replace \"target_variable\" with the actual name of the target variable column\n",
    "# Splitting the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], \n",
    "test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "n the above code:\n",
    "\n",
    "X represents the feature matrix (all the columns except the target variable).\n",
    "y represents the target variable.\n",
    "test_size=0.3 specifies that 30% of the data will be used for testing, and 70% will be used for training. \n",
    "You can adjust this ratio as needed.\n",
    "random_state=42 sets a specific random seed for reproducibility. Change this value or remove it to have a \n",
    "different random split each time.\n",
    "After executing the code, you will have the following variables:\n",
    "\n",
    "X_train: The training set features.\n",
    "X_test: The test set features.\n",
    "y_train: The training set target variable.\n",
    "y_test: The test set target variable.\n",
    "\n",
    "You can then use these sets for training and evaluating your machine learning models.\n",
    "\n",
    "\"\"\"Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for \n",
    "each tree. Use the default values for other hyperparameters.\"\"\"\n",
    "\n",
    "Ans:\n",
    "To train a Random Forest Classifier on the training set with 100 trees and a maximum depth of 10 for each \n",
    "tree, you can use the RandomForestClassifier class from scikit-learn. Heres an example of how to do it:\n",
    "    \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming you have split the dataset into X_train and y_train\n",
    "\n",
    "# Instantiate the Random Forest Classifier with the desired hyperparameters\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "In the above code:\n",
    "\n",
    "n_estimators=100 specifies the number of trees in the random forest. You can adjust this value based on \n",
    "your specific requirements.\n",
    "max_depth=10 sets the maximum depth of each decision tree in the random forest. This parameter controls \n",
    "the complexity of the trees and helps prevent overfitting. Adjust this value as needed.\n",
    "\n",
    "X_train represents the training set features.\n",
    "y_train represents the training set target variable.\n",
    "After executing the code, the rf_classifier object will be trained on the training set using the specified \n",
    "hyperparameters.\n",
    "\n",
    "You can then use this trained classifier to make predictions on new data or evaluate its performance on \n",
    "the test set.\n",
    "\n",
    "\"\"\"Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 \n",
    "score.\"\"\"\n",
    "Ans: import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df[\"target\"], test_size=0.25)\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the precision of the classifier\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the recall of the classifier\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the F1 score of the classifier\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n",
    "\n",
    "\n",
    "\"\"\"Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\n",
    "disease risk. Visualise the feature importances using a bar chart.\"\"\"\n",
    "Ans: import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_importance import feature_importances_\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df[\"target\"], test_size=0.25)\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the precision of the classifier\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the recall of the classifier\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the F1 score of the classifier\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Get the top 5 features\n",
    "top_5_features = df.columns[sorted_idx][:5]\n",
    "\n",
    "# Print the top 5 features\n",
    "print(\"Top 5 features:\", top_5_features)\n",
    "\n",
    "# Visualize the feature importances using a bar chart\n",
    "plt.barh(top_5_features, feature_importances[sorted_idx][:5])\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try\n",
    "different values of the number of trees, maximum depth, minimum samples split, and minimum samples\n",
    "leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters.\"\"\"\n",
    "Ans: import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df[\"target\"], test_size=0.25)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "params = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [5, 10, 15],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), params, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Make predictions on the test set using the best parameters\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\"\"\"Q7. Report the best set of hyperparameters found by the search and the corresponding performance\n",
    "metrics. Compare the performance of the tuned model with the default model.\"\"\"\n",
    "\n",
    "Ans: import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load the data\n",
    "X = np.loadtxt('data.txt', delimiter=',')\n",
    "y = np.loadtxt('labels.txt', delimiter=',')\n",
    "\n",
    "# Create a grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'n_estimators': [10, 100, 1000],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a RandomForestClassifier model\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters found by the search\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "\n",
    "# Train a model using the best hyperparameters\n",
    "clf = RandomForestClassifier(**best_params)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = clf.predict(X)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "\n",
    "# Print the performance metrics\n",
    "print('Accuracy:', accuracy)\n",
    "print('F1 score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "\"\"\"Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot the\n",
    "decision boundaries on a scatter plot of two of the most important features. Discuss the insights and\n",
    "limitations of the model for predicting heart disease risk.\"\"\"\n",
    "Ans: import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "Load the data\n",
    "X = np.loadtxt('data.txt', delimiter=',')\n",
    "y = np.loadtxt('labels.txt', delimiter=',')\n",
    "\n",
    "Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "Create a RandomForestClassifier model\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "Get the feature importances\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "Get the two most important features\n",
    "most_important_features = np.argsort(feature_importances)[-2:]\n",
    "\n",
    "Plot the decision boundaries on a scatter plot of the two most important features\n",
    "plt.scatter(X_train[:, most_important_features[0]], X_train[:, most_important_features[1]], \n",
    "c=y_train, cmap='cool')\n",
    "plt.xlabel(f'Feature {most_important_features[0]}')\n",
    "plt.ylabel(f'Feature {most_important_features[1]}')\n",
    "plt.title('Decision Boundaries of Random Forest Classifier')\n",
    "plt.show()\n",
    "\n",
    "Evaluate the performance of the model on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "Print the performance metrics\n",
    "print('Accuracy:', accuracy)\n",
    "print('F1 score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "Insights\n",
    "The decision boundaries of the random forest classifier are non-linear, which suggests that the \n",
    "relationship between the features and the target variable is non-linear.\n",
    "The model is able to achieve a high accuracy on the testing set, suggesting that it is able to generalize \n",
    "well to new data.\n",
    "The model is able to identify two important features for predicting heart disease risk: age and systolic \n",
    "blood pressure.\n",
    "Limitations\n",
    "The model is not able to identify all of the important features for predicting heart disease risk.\n",
    "The model is not able to explain why certain features are important for predicting heart disease risk.\n",
    "The model is not able to predict the risk of heart disease for individuals with very high or very low \n",
    "values for the two most important features.\n",
    "The insights and limitations of the model suggest that it can be a useful tool for predicting heart \n",
    "disease risk, but it is important to note that it is not a perfect tool. The model should be used in \n",
    "conjunction with other tools, such as clinical judgment, to make decisions about individual patients.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
